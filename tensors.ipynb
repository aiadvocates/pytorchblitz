{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Tensors and Operations\n",
    "\n",
    "**Tensor** is the basic computational unit in PyTorch. It is very similar to **NumPy array**, and supports similar operations. However, there are two very important features of Torch tensors that make them especially useful for training large-scale neural networks:\n",
    "\n",
    "* Tensor operations can be performed on GPU using CUDA\n",
    "* Tensor operations support automatic differentiation using [AutoGrad](autograd.ipynb)\n",
    "\n",
    "Conversion between Torch tensors and NumPy arrays can be done easily:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), Array=[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "np_array = np.arange(10)\n",
    "tensor = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"Tensor={tensor}, Array={tensor.numpy()}\")"
   ]
  },
  {
   "source": [
    "**Note:** When using CPU for computations, tensors converted from arrays share the same memory for data. Thus, changing the underlying array will also affect the tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Creating Tensors\n",
    "\n",
    "The fastest way to create a tensor is to define an *uninitialized* tensor - the values of this tensor are not set, and depend on the whatever data was there in memory:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(3,6)\n",
    "print(x)"
   ]
  },
  {
   "source": [
    "In practice, we ofter want to create tensors initialized to some values, such as zeros, ones or random values. Note that you can also specify the type of elements using `dtype` parameter, and chosing one of `torch` types:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,5)\n",
    "print(x)\n",
    "y = torch.zeros(3,5,dtype=torch.int)\n",
    "print(y)\n",
    "z = torch.ones(3,5,dtype=torch.double)\n",
    "print(z)"
   ]
  },
  {
   "source": [
    "You can also create random tensors with values sampled from different distributions, as described [in documentation](https://pytorch.org/docs/stable/torch.html#random-sampling).\n",
    "\n",
    "Similarly to NumPy, you can use `eye` to create a diagonal identity matrix:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.eye(10))"
   ]
  },
  {
   "source": [
    "You can also create new tensors with the same properties or size as existing tensors:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.new_ones(2,2)) # new_ method allows specifying new size\n",
    "print(torch.zeros_like(x,dtype=torch.long)) # _like method supports overriding dtype"
   ]
  },
  {
   "source": [
    "Size of the tensor can be obtained using `.size()` method, which returns a tuple-like object:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.size())"
   ]
  },
  {
   "source": [
    "### Tensor Operations\n",
    "\n",
    "Tensors support all basic arithmetic operations, which can be specified in different ways:\n",
    "* Using operators, such as `+`, `-`, etc.\n",
    "* Using functions such as `add`, `mult`, etc. Functions can either return values, or store them in the specified ouput variable (using `out=` parameter)\n",
    "* In-place operations, which modify one of the arguments. Those operations have `_` appended to their name, eg. `add_`.\n",
    "\n",
    "Complete reference to all tensor operations can be found [in documentation](https://pytorch.org/docs/stable/torch.html).\n",
    "\n",
    "Let us see examples of those operations on two tensors, `x` and `y`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,5)\n",
    "y = torch.randn(3,5)"
   ]
  },
  {
   "source": [
    "#### Using operator notation\n",
    "\n",
    "We can use overloaded arithmetic operators, such as `+` and `*`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x*y)"
   ]
  },
  {
   "source": [
    "Note, that `*` means elementwise product, and not the matrix product. To compute matrix product, we need to use `matmul` function, as shown below.\n",
    "\n",
    "#### Using functions\n",
    "\n",
    "While only some operations are available as Python operators, [many more functions](https://pytorch.org/docs/stable/torch.html#math-operations) can be specified using the full name. In the example below, `t` transposes the matrix, and `matmul` means matrix multiplication:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(x,y.t())"
   ]
  },
  {
   "source": [
    "Simple operations (addition, multiplication, etc.) also have corresponsing functions, and can be called either as methods, or as functions: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.add(y))\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "source": [
    "Sometimes it may be more convenient to store the result into specified variable, instead of returning it from a function. In this case you can use `out=` parameter:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.add(x,y,out=z)\n",
    "print(z)"
   ]
  },
  {
   "source": [
    "#### In-place operations\n",
    "\n",
    "When training neural networks, you often need to **modify** the weights, i.e. perform some operation and then store the result into the original variable. Those operations are called **in-place operations**, and they are marked by the `_` symbol at the end of their name: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "source": [
    "### Resizing and Indexing\n",
    "\n",
    "Very often you need to change the shape of the tensor without modifying its valies, eg. to add an extra dimension. To do that, you can use `view` method, which provides a **view** to the same in-memory values using different dimensions:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(x.view(5,3,1))\n",
    "print(x.view(5,-1))"
   ]
  },
  {
   "source": [
    "Note that the number of elements in a view should be the same as in the original tensor, and that you can use `-1` in one of the dimensions to figure out this dimension automatically."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Note:** `view` is similar to `reshape` operation in NumPy. There is also a `reshape` method available in PyTorch, and it is more powerful than `view`, because it can also reshape non-contiguous arrays by copying them to the new shape. However, in vast majority of cases you can use `view` and make sure that no data copying occurs, and the operation is always efficient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Tensors support all slicing operations that exist in NymPy:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0], x[:,0], x[...,1])"
   ]
  },
  {
   "source": [
    "If you have a one-element tensor, for example, after aggregating all values of the tensor into one value, you can convert it to a Python numerical value using `item()`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.sum().item())"
   ]
  },
  {
   "source": [
    "### GPU Computations\n",
    "\n",
    "One of the major benefits of using PyTorch is the ability to perform tensor operations on GPU. To do that, we need to explicitly **move** tensors to GPU using `.to` method.\n",
    "\n",
    "In most of the cases, we check for the availability of GPU on our machine, and define the `device` object accordingly. Then we move all tensors to that device before performing the computations:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Doing computations on {}\".format(device))\n",
    "\n",
    "x = torch.randn(3,5,device=device)\n",
    "y = torch.ones_like(x)\n",
    "y = y.to(device)\n",
    "z = x+y # this is performed on GPU if it is available\n",
    "print(z)\n",
    "print(z.to(\"cpu\",torch.double))"
   ]
  },
  {
   "source": [
    "In the last operation, when we move the tensor back to the CPU, we can also change the `dtype`. This does not result in additional computational time, because we need to copy and transform the data when moving it from GPU anyway."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}